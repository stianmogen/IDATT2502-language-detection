{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3bb70f1-c862-4dc3-99bb-07486721b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_train.txt', 'labels.csv', 'y_train.txt', 'x_test.txt', 'y_test.txt', 'README.md', '.ipynb_checkpoints', 'urls.txt']\n",
      "cpu\n",
      "(117500, 1)\n",
      "(117500, 1)\n",
      "Example:\n",
      "LANG = est\n",
      "TEXT = Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch # Deep learning framework\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "INPUTDIR = 'input'\n",
    "print(os.listdir(f'{INPUTDIR}'))\n",
    "\n",
    "#Init random seed to get reproducible results\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Constants\n",
    "\n",
    "NUM_CLASSES = 235\n",
    "INPUT_DIR = \"input/\"\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "with open(os.path.join(INPUT_DIR, \"x_train.txt\"), encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "x_train = data.split('\\n')\n",
    "\n",
    "with open(os.path.join(INPUT_DIR, \"x_test.txt\"), encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "x_test = data.split('\\n')\n",
    "\n",
    "with open(os.path.join(INPUT_DIR, \"y_train.txt\"), encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "y_train = data.split('\\n')\n",
    "\n",
    "with open(os.path.join(INPUT_DIR, \"y_test.txt\"), encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "y_test = data.split('\\n')\n",
    "\n",
    "x_train.pop(-1)\n",
    "x_test.pop(-1)\n",
    "y_train.pop(-1)\n",
    "y_test.pop(-1)\n",
    "\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=['sentence'])\n",
    "y_train = pd.DataFrame(y_train, columns=['language'])\n",
    "\n",
    "x_test = pd.DataFrame(x_test, columns=['sentence'])\n",
    "y_test = pd.DataFrame(y_test, columns=['language'])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "\n",
    "print('Example:')\n",
    "print('LANG =', y_train['language'].iloc[0])\n",
    "print('TEXT =', x_train['sentence'].iloc[0])\n",
    "\n",
    "x_train_sentence = x_train['sentence']\n",
    "y_train_language = y_train['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92fcf23-4791-44c8-8628-ae56c81e62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token] = len(self.idx2token) - 1\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3acf6fa0-744f-49f4-b083-0546fbc17280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 10808 UTF characters\n",
      "Labels: 235 languages\n"
     ]
    }
   ],
   "source": [
    "char_vocab = Dictionary()\n",
    "pad_token = '<pad>' # reserve index 0 for padding\n",
    "unk_token = '<unk>' # reserve index 1 for unknown token\n",
    "pad_index = char_vocab.add_token(pad_token)\n",
    "unk_index = char_vocab.add_token(unk_token)\n",
    "\n",
    "# join all the training sentences in a single string\n",
    "# and obtain the list of different characters with set\n",
    "chars = set(''.join(x_train_sentence))\n",
    "for char in sorted(chars):\n",
    "    char_vocab.add_token(char)\n",
    "print(\"Vocabulary:\", len(char_vocab), \"UTF characters\")\n",
    "\n",
    "lang_vocab = Dictionary()\n",
    "# use python set to obtain the list of languages without repetitions\n",
    "languages = set(y_train_language)\n",
    "for lang in sorted(languages):\n",
    "    lang_vocab.add_token(lang)\n",
    "print(\"Labels:\", len(lang_vocab), \"languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87af426b-78e7-43a5-b0fe-766976b39f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k -> 77\n",
      "est -> 52\n",
      "est Klement Go\n",
      "52 [45 78 71 79 71 80 86  2 41 81]\n"
     ]
    }
   ],
   "source": [
    "#From token or label to index\n",
    "print('k ->', char_vocab.token2idx['k'])\n",
    "print('est ->', lang_vocab.token2idx['est'])\n",
    "print(y_train_language[0], x_train_sentence[0][:10])\n",
    "x_train_idx = [np.array([char_vocab.token2idx[c] for c in line]) for line in x_train_sentence]\n",
    "y_train_idx = np.array([lang_vocab.token2idx[lang] for lang in y_train_language])\n",
    "print(y_train_idx[0], x_train_idx[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e733b20-b3bc-4307-a2cf-e44a3822ac6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
